p8105_hw5_jj3564
================
Kate
2025-11-14

## problem 1

``` r
# Define a function that checks if at least two people share a birthday
dup_bday <- function(n) {
  # Randomly assign birthdays (1–365) to n people, allowing duplicates
  bdays <- sample(1:365, n, replace = TRUE)
  # Return TRUE if there is any duplicate birthday, FALSE otherwise
  any(duplicated(bdays))
}

# Define a function to estimate the probability of shared birthdays
prob_bday <- function(n, sims = 1e4) {
  # Repeat the birthday simulation 'sims' times
  # Each run returns TRUE/FALSE; taking the mean gives the estimated probability
  mean(replicate(sims, dup_bday(n)))
}

# Create a tibble with group sizes from 2 to 50
# For each group size, compute the probability using 'prob_bday'
results <- tibble(
  n = 2:50,
  prob = map_dbl(n, prob_bday)
)

# Plot the probability curve using ggplot2

ggplot(results, aes(x = n, y = prob)) +
  geom_line(linewidth = 1.1) +  # Draw line
  geom_point() +                  # Add data points
  scale_y_continuous(labels = scales::percent) +   # Convert y-axis to %
  labs(
    title = "Probability that at least two people share a birthday",
    x = "Group size",
    y = "Probability"
  )
```

![](p8105_hw5_jj3564_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->

``` r
# Result Interpretation:
# 
# The resulting plot visualizes the "Birthday Paradox" — 
# how the probability that at least two people share the same birthday 
# increases with group size.
# 
# The curve starts near 0% when only 2–5 people are in the group, 
# but rises rapidly as the group becomes larger. 
# Around 23 people, the probability exceeds 50%,
# Around 50 people, the probablity is next to 100%
# showing that even in a relatively small group, 
# there is a surprisingly high chance that two people share a birthday.
# 
# This counterintuitive result illustrates how random combinations 
# grow quickly with group size, and give us an intution that though 365 days, only 50 
#   people can nearly guarantee a repetition
```

\##Problem 2

``` r
# Design parameters
n <- 30
sigma <- 5
mus <- 0:6
n_sim <- 5000

# Run simulations for each true mean
sim_df <- tibble(mu = mus) %>%
  mutate(
    res = map(mu, ~ {
      # For each mu, run 5000 simulated t-tests
      replicate(n_sim, {
        x <- rnorm(n, mean = .x, sd = sigma)
        tidy(t.test(x, mu = 0)) %>%
          select(estimate, p.value)
      }, simplify = FALSE) %>%
        bind_rows()
    })
  ) %>%
  unnest(res)

# Compute power and mean estimates
power_df <- sim_df %>%
  group_by(mu) %>%
  summarise(
    power = mean(p.value < 0.05),
    mean_all = mean(estimate),
    mean_sig = mean(estimate[p.value < 0.05]),
    .groups = "drop"
  )

# ---- Plot 1: Power vs Effect Size ----
ggplot(power_df, aes(mu, power)) +
  geom_line(color = "#7B4397", linewidth = 1.1) +
  geom_point(color = "#7B4397") +
  labs(
    title = "Power increases with true μ",
    x = "True mean (μ)",
    y = "Power"
  )
```

![](p8105_hw5_jj3564_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

``` r
# ---- Plot 2: Average Estimate vs True Mean ----
power_df %>%
  pivot_longer(c(mean_all, mean_sig),
               names_to = "type", values_to = "mean_est") %>%
  mutate(type = recode(type,
                       mean_all = "All samples",
                       mean_sig = "Samples where null rejected")) %>%
  ggplot(aes(mu, mean_est, color = type)) +
  geom_line(linewidth = 1.1) +
  geom_point() +
  labs(
    title = "Average estimate vs true mean",
    x = "True mean (μ)",
    y = "Average of sample means",
    color = NULL
  )
```

![](p8105_hw5_jj3564_files/figure-gfm/unnamed-chunk-2-2.png)<!-- -->
\###interpretation:As the true mean increases, the power of the test
rises steadily from about 5% to nearly 100%. Larger effect sizes make it
easier to reject the null hypothesis. \###From the second plot, the
average estimated mean (μ̂) among samples where the null was rejected
differs from the true μ, particularly when the effect size is small. For
smaller true means, the line representing significant samples lies above
both the overall average and the true μ, indicating a positive bias in
the subset of significant results. \###This bias arises because
conditioning on rejection introduces selection bias: when μ is small,
only samples with unusually large sample means (those far above the true
μ) achieve p-values below 0.05. These overestimates dominate the
rejected group, inflating the average μ̂.As μ grows larger, power
approaches one—most samples become significant—so the selection bias
diminishes. The two lines then converge, and both averages approach the
true μ.

\##problem3

``` r
#  Read and summarize data
homicides<-read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
city_stats <- homicides %>%
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved_flag = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>%
  group_by(city_state) %>%
  summarise(
    total_cases = n(),
    unsolved_cases = sum(unsolved_flag),
    .groups = "drop"
  )

#  Baltimore test 
baltimore_result <- city_stats %>%
  filter(city_state == "Baltimore, MD") %>%
  mutate(
    test_output = map2(unsolved_cases, total_cases, \(x, n) tidy(prop.test(x, n)))
  ) %>%
  unnest(test_output) %>%
  select(estimate, conf.low, conf.high)

baltimore_result
```

    ## # A tibble: 1 × 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

``` r
#  3. Run prop.test for every city
city_results <- city_stats %>%
  mutate(
    test_output = map2(unsolved_cases, total_cases, \(x, n) tidy(prop.test(x, n)))
  ) %>%
  unnest(test_output) %>%
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `test_output = map2(...)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
#  4. Plot unsolved homicide proportions 
city_results %>%
  arrange(desc(estimate)) %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(estimate, city_state)) +
  geom_point(color = "#8E44AD", size = 2.2) +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), color = "#8E44AD", width = 0.15) +
  labs(
    title = "Unsolved Homicide Proportions by City",
    subtitle = "Each point represents an estimated proportion (95% CI)",
    x = "Estimated Proportion of Unsolved Cases",
    y = NULL
  ) +
  theme_light(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", color = "#4A235A"),
    plot.subtitle = element_text(size = 11, color = "gray30"),
    panel.grid.minor = element_blank()
  ) +
  coord_flip()
```

![](p8105_hw5_jj3564_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->
